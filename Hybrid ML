import numpy as np
from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

# Sample data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Replace X, y with your actual data

# Assuming you have obtained the best hyperparameters
best_ada_params = [best_n_estimators_ada, best_learning_rate_ada]  # Replace with actual values
best_rf_params = [best_max_depth_rf, best_n_estimators_rf]  # Replace with actual values
best_xgb_params = [best_learning_rate_xgb, best_n_estimators_xgb, best_max_depth_xgb]  # Replace with actual values

# Define the optimized models
ada_reg = AdaBoostRegressor(n_estimators=best_ada_params[0], learning_rate=best_ada_params[1], random_state=42)
rf_reg = RandomForestRegressor(max_depth=best_rf_params[0], n_estimators=best_rf_params[1], random_state=42)
xgb_reg = XGBRegressor(learning_rate=best_xgb_params[0], n_estimators=best_xgb_params[1], max_depth=best_xgb_params[2], random_state=42)

# Train the models
ada_reg.fit(X_train, y_train)
rf_reg.fit(X_train, y_train)
xgb_reg.fit(X_train, y_train)

# Make predictions
ada_pred = ada_reg.predict(X_test)
rf_pred = rf_reg.predict(X_test)
xgb_pred = xgb_reg.predict(X_test)

# Create a hybrid ensemble by averaging predictions
hybrid_pred = (ada_pred + rf_pred + xgb_pred) / 3

# Evaluate the performance of the hybrid ensemble
hybrid_mse = mean_squared_error(y_test, hybrid_pred)
print("Hybrid Ensemble MSE:", hybrid_mse)
