import numpy as np
from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler

# Sample data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Replace X, y with your actual data

# Assuming you have obtained the best hyperparameters
best_ada_params = [best_n_estimators_ada, best_learning_rate_ada]  # Replace with actual values
best_rf_params = [best_max_depth_rf, best_n_estimators_rf]  # Replace with actual values
best_xgb_params = [best_learning_rate_xgb, best_n_estimators_xgb, best_max_depth_xgb]  # Replace with actual values

# Define the optimized models
ada_reg = AdaBoostRegressor(n_estimators=best_ada_params[0], learning_rate=best_ada_params[1], random_state=42)
rf_reg = RandomForestRegressor(max_depth=best_rf_params[0], n_estimators=best_rf_params[1], random_state=42)
xgb_reg = XGBRegressor(learning_rate=best_xgb_params[0], n_estimators=best_xgb_params[1], max_depth=best_xgb_params[2], random_state=42)

# Train the models
ada_reg.fit(X_train, y_train)
rf_reg.fit(X_train, y_train)
xgb_reg.fit(X_train, y_train)

# Make predictions
ada_pred = ada_reg.predict(X_test)
rf_pred = rf_reg.predict(X_test)
xgb_pred = xgb_reg.predict(X_test)

# Stack predictions as features for the meta-learner (ELM)
stacked_features = np.column_stack((ada_pred, rf_pred, xgb_pred))

# Scale features for ELM (optional but recommended for neural networks)
scaler = StandardScaler()
stacked_features_scaled = scaler.fit_transform(stacked_features)

# Define the ELM meta-learner
elm_meta_learner = MLPRegressor(hidden_layer_sizes=(10,), activation='relu', random_state=42)

# Train the ELM meta-learner
elm_meta_learner.fit(stacked_features_scaled, y_test)

# Make predictions using the meta-learner
meta_learner_pred = elm_meta_learner.predict(stacked_features_scaled)

# Evaluate the performance of the hybrid ensemble with ELM meta-learner
hybrid_mse = mean_squared_error(y_test, meta_learner_pred)
print("Hybrid Ensemble with ELM Meta-Learner MSE:", hybrid_mse)
